# -*- coding: utf-8 -*-
"""GANSubspaceOptuna.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vzDg9llWiSHqD8e8L10e1TEH1HVIyMMm
"""

# -*- coding: utf-8 -*-
"""SubSpaceJSOptuna.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I9vXOXmCfmNA6dAk1ZJagTkIx6YJVkS7
"""

# -*- coding: utf-8 -*-
"""SubSpaceJS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13TKGc6sVjhS5Ok9MTliNmm8aGU8X4xX8
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf

from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from tensorflow.keras import layers, losses
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Model

import random
import sys
import time
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.backend as keras_backend

from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import AveragePooling2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import Dropout
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import concatenate
from sklearn.feature_selection import mutual_info_regression
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize
from operator import add
from numpy import asarray
import math
import scipy
import seaborn as sns
import networkx as nx
import numpy as np
import optuna as optuna
import json
#original =tf.zeros(3,4)
#test = tf.zeros(3,4)
#original_labels = tf.zeros(3,4)
#log_steps=1000
#data = [[10,20,20,30,40,50],[10,20,20,30,40,50]]
import math
def kl_divergence(p, q):
	return sum(p[i] * math.log((p[i]/q[i]),2) for i in range(len(p)))
 
# calculate the js divergence
def js_divergence(p, q):
	m = 0.5 * (p + q)
	return 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)

####for discrete distributions

def compute_probs(data, n=10, lim = 100): 
  h, e = np.histogram(data, bins=n, range=(0,lim))
  p = h/sum(h)
  return e, p, h
def support_intersection(p, q): 
  sup_int = (
        list(
            filter(
                lambda x: (x[0]!=0) & (x[1]!=0), zip(p, q)
            )
        )
    )
  return sup_int
def get_probs(list_of_tuples): 
  p = np.array([p[0] for p in list_of_tuples])
  q = np.array([p[1] for p in list_of_tuples])
  return p, q
def kl_div(p, q): 
  return np.sum(p * np.log(p/q))
def divergence(data1, data2):
  e1, p1 , h1= compute_probs(data1,lim=max(data1))

  e2, p2 , h2 = compute_probs(data2, lim=max(data2))
  
  list_of_tuples = []
  if e1[len(e1)-1]>e2[len(e2)-1]:
    e3, p3, h3 = compute_probs(data2, lim=max(data1))
    list_of_tuples = support_intersection(p1, p3)
  else:
    e3, p3, h3 = compute_probs(data1,lim=max(data2))
    list_of_tuples = support_intersection(p3, p2)
 
  #list_of_tuples.append((0.75,.25))
  #print("List of tuples")
  #print(list_of_tuples)
  p, q = get_probs(list_of_tuples)
  return kl_divergence(p, q)
def computeJS(data1, data2):
  e1, p1 , h1= compute_probs(data1,lim=max(data1))

  e2, p2 , h2 = compute_probs(data2, lim=max(data2))
  
  list_of_tuples = []
  if e1[len(e1)-1]>e2[len(e2)-1]:
    e3, p3, h3 = compute_probs(data2, lim=max(data1))
    list_of_tuples = support_intersection(p1, p3)
  else:
    e3, p3, h3 = compute_probs(data1,lim=max(data2))
    list_of_tuples = support_intersection(p3, p2)
 
  #list_of_tuples.append((0.75,.25))
  #print("List of tuples")
  #print(list_of_tuples)
  p, q = get_probs(list_of_tuples)
  return js_divergence(p, q)


####MMD
def mmd_linear(X, Y):
  delta = X.mean(0) - Y.mean(0)
  return delta.dot(delta.T)


#### for continouous distributions
def compute_gamma_parameters(data): 
    mean = np.mean(data)
    variance = np.var(data)
    alpha = (mean**2)/variance
    beta = (mean)/(variance)
    theta = 1/beta
    return alpha, beta
def kl_divergence_generalized_gamma(alpha_1, beta_1, alpha_2, beta_2, p1=1, p2=1):
    """
    Computes the Kullback-Leibler divergence 
    between two gamma distributions
    """
    theta_1 = 1/beta_1
    theta_2 = 1/beta_2
    
    """print("beta 1", beta_1);
    print("beta 2", beta_2);
    print("theta 1", theta_1);
    print("theta 2", theta_2);"""
    a = p1*(theta_2**alpha_2)*scipy.special.gamma(alpha_2/p2)
    b = p2*(theta_1**alpha_1)*scipy.special.gamma(alpha_1/p1)
    c = (((scipy.special.digamma(alpha_1/p1))/p1) + 
         np.log(theta_1))*(alpha_1 - alpha_2)
    d = scipy.special.gamma((alpha_1+p2)/p1)
    e = scipy.special.gamma((alpha_1/p1))
    f = (theta_1/theta_2)**(p2)
    g = alpha_1/p1
    
    resdue1 = np.log(a/(b+1e-60))
    resdue2 = c
    resdue3 = (d/e)*f 
    resdue4 = g
    
    """
    print("resdue1 ")
    print(resdue1)
    print("resdue2 ")
    print(resdue2)
    print("resdue3 ")
    print(resdue3)
    print("resdue4 ")
    print(resdue4)
    """
    
    if np.isnan(resdue1):
      resdue1 = 0
    if np.isnan(resdue2):
      resdue2 = 0
    if np.isnan(resdue3):
      resdue3 = 0
    if np.isnan(resdue4):
      resdue4 = 0
      
    """ 
    if type(resdue1)!= float and type(resdue1)!= int:
      resdue1 = 0
    if type(resdue2)!= float and type(resdue2)!= int:
      resdue2 = 0
    if type(resdue3)!= float and type(resdue3)!= int:
      resdue3 = 0
    if type(resdue4)!= float and type(resdue4)!= int:
      resdue4 = 0
    
    
    print("Total resdue ")
    print(resdue1 + resdue2 + resdue3 + resdue4)
    """
    
    return resdue1 + resdue2 + resdue3 + resdue4
def compute_kl(data, data1):
  alpha, beta =  compute_gamma_parameters(data)
  alpha1, beta1 = compute_gamma_parameters(data1)
  d1 =  kl_divergence_generalized_gamma(alpha, beta, alpha1, beta1, p1=1, p2=1)
  return d1
def compute_js(p, q):
  l1 = list( map( add, p, q ) )
  m = [x / 2 for x in l1]
  return 0.5 * compute_kl(p, m) + 0.5 * compute_kl(q, m)

def MICalculator(Data1, subspaceofData1, Data1labels):
  reg1 = mutual_info_regression(Data1, Data1labels)
  reg2 = mutual_info_regression(subspaceofData1, Data1labels)
  MIR = 0.0
  for i in range(len(reg1)):
    MIR += reg1[i]-reg2[i]
  return MIR
  
####Normalization
def newNorm(X):
  q = max(X) - min(X)
  #print("q ",q)
  a = [(x - min(X))/(q+1e-60) for x in X]
  #print("a ",a)
  return a
####NewNorm
def newNorm2(X):
  q = np.max(X) - np.min(X)
  q1 = np.min(X)
  a = [((x - q1)/(q+1e-60))+1e-60 for x in X]
  #a = [(x + abs(q) + 1e-60) for x in X]
  return a

####Data Generator

def dataGene(purpose, x1, X1, lb1):
  if (purpose=="train"):
    #i=random.randint(0,2000)
    #org = x[i:(i+1000)]
    org = x1
    #labels = y[i:(i+1000)]
    labels = lb1
    #i=random.randint(0,20000)
    #dum = X[i:(i+30000)]
    dum = X1
  
  return org, dum, labels

class Encoder(tf.keras.layers.Layer):
  def __init__(self, intermediate_dim, numOfLayersE, neuronsE, activationE):
    super(Encoder, self).__init__()
    self.array=[]
    self.nL = numOfLayersE
    self.neurons = neuronsE
    self.idim = intermediate_dim
    self.actF = activationE
    initializer = tf.keras.initializers.RandomUniform(minval=0.3, maxval=0.6)
    #layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)
    self.hidden_layer = tf.keras.layers.Dense(
      units=neuronsE,
      activation=activationE
      #,kernel_initializer=initializer
    )
    for i in range(self.nL):
      self.array.append(Dense(neuronsE, activation=activationE))
    self.output_layer = tf.keras.layers.Dense(
      units=intermediate_dim,
      activation=activationE
      #,kernel_initializer=initializer
    )
  def call(self, input_features):
    encoded = self.hidden_layer(input_features)
    for i in range(self.nL):
      encoded=self.array[i](encoded)
    #print("Activation ",activation)
    return self.output_layer(encoded)


class Decoder(tf.keras.layers.Layer):
  def __init__(self, intermediate_dim, original_dim, numOfLayersD, neuronsD, activationD):
    super(Decoder, self).__init__()
    self.array=[]
    self.nL = numOfLayersD
    self.neurons = neuronsD
    self.odim = original_dim
    initializer = tf.keras.initializers.RandomUniform(minval=0.3, maxval=0.6)
    #layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)
    self.hidden_layer = tf.keras.layers.Dense(
      units=intermediate_dim,
      activation=activationD
      #,kernel_initializer=initializer
    )
    for i in range(self.nL):
      self.array.append(Dense(neuronsD, activation=activationD))
    self.output_layer = tf.keras.layers.Dense(
      units=original_dim,
      activation=activationD
      #,kernel_initializer=initializer
    )
  def call(self, code):
    decoded = self.hidden_layer(code)
    for i in range(self.nL):
      decoded=self.array[i](decoded)
    return self.output_layer(decoded)

class Autoencoder(tf.keras.Model):
  def __init__(self, intermediate_dim, original_dim1, original_dim2, numOfLayers, neurons, activation):
    super(Autoencoder, self).__init__()
    self.encoder1 = Encoder(intermediate_dim=intermediate_dim, numOfLayersE=numOfLayers, neuronsE=neurons, activationE=activation)
    self.encoder2 = Encoder(intermediate_dim=intermediate_dim, numOfLayersE=numOfLayers, neuronsE=neurons, activationE=activation)
    self.decoder1 = Decoder(intermediate_dim=intermediate_dim, original_dim=original_dim1, numOfLayersD=numOfLayers, neuronsD=neurons, activationD=activation)
    self.decoder2 = Decoder(intermediate_dim=intermediate_dim, original_dim=original_dim2, numOfLayersD=numOfLayers, neuronsD=neurons, activationD=activation)
  
  def call(self, input_features1, input_features2):
    code1 = self.encoder1(input_features1)
    #print("Code 1", code1)
    code2 = self.encoder2(input_features2)
    #print("COde 2", code2)
    reconstructed1 = self.decoder1(code1)
    reconstructed2 = self.decoder2(code2)
    return reconstructed1, reconstructed2
  def getEncoded1(self, input_features):
    #input_features = tf.convert_to_tensor(input_features, dtype=tf.float64)
    return self.encoder1(input_features)
  def getEncoded2(self, input_features):
    #input_features = tf.convert_to_tensor(input_features, dtype=tf.float64)
    return self.encoder2(input_features)
  def get_config(self):
      return {"intermediate_dim": self.encoder1.idim,"original_dim1": self.decoder1.odim,"original_dim2": self.decoder2.odim, "numOfLayers": self.encoder1.nL,"neurons": self.encoder1.neurons, "activation": self.encoder1.actF }
  @classmethod
  def from_config(cls, config):
      return cls(**config)

def make_discriminator_model(intermediate_dim):
    model = tf.keras.Sequential()

    model.add(tf.keras.layers.Dense(
      units=intermediate_dim
      #,kernel_initializer=initializer
    ))
    model.add(tf.keras.layers.LeakyReLU())
    #model.add(tf.keras.layers.Dropout(0.3))

    model.add(tf.keras.layers.Dense(units=intermediate_dim/2))
    model.add(layers.LeakyReLU())
    #model.add(layers.Dropout(0.3))

    model.add(layers.Dense(1))

    return model

def discriminator_loss(real_output, fake_output):
  cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)
  real_loss = cross_entropy(tf.ones_like(real_output), real_output)
  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
  total_loss = real_loss + fake_loss
  return total_loss
def generator_loss(fake_output):
  cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)
  return cross_entropy(tf.ones_like(fake_output), fake_output)

def loss(model, model2, original, test, Labels, lossType, alpha1, alpha2, beta1, beta2, gamma1, gamma2, delta1, delta2):
  value1, value2 = model(original, test)
  #print("Loss 1.0 value1", value1)
  #print("Loss 1.0 original", original)
  value1 = tf.cast(value1, dtype=tf.float64)
  value2 = tf.cast(value2, dtype=tf.float64)	
  ####Mean Squared error####################
  
  sq1 = tf.square(tf.subtract(value1, original))
  sq1 = tf.reduce_mean(sq1)
  #print("Sq1 ",sq1);
  sq2 = tf.square(tf.subtract(value2, test))
  sq2 = tf.reduce_mean(sq2)
  #print("Sq2 ", sq2);


  ###Subspace values of source and target############
  enc1 = model.getEncoded1(original)
  enc2 = model.getEncoded2(test)
  
  """
  print("enc1  ")
  print(enc1)
  """
  
  ##normalizing subspace values by feature wise
  x = np.transpose(enc1)
  y = np.transpose(enc2)

  new_matrix1 = np.zeros((x.shape[0], x.shape[1]))
  new_matrix2 = np.zeros((y.shape[0], y.shape[1]))
  for i in range(len(new_matrix1)):
    xN = newNorm2(x[i])
    yN = newNorm2(y[i])
    new_matrix1[i]= xN
    new_matrix2[i]= yN

  x = new_matrix1.T
  y = new_matrix2.T
  
  #print(x)


  ###PCA Initialization
  pca1 = PCA(n_components=1)
  pca2 = PCA(n_components=1)
  pca3 = PCA(n_components=1)
  pca4 = PCA(n_components=1) 
  
  ###Projecting source, subspace of Source, target, subspace of target into one component########
  OrgPrincipalComponents = pca1.fit_transform(original)
  EncPrincipalComponents = pca2.fit_transform(x)
  TestPrincipalComponents = pca3.fit_transform(test)
  Enc2PrincipalComponents = pca4.fit_transform(y)
 
  ####Normalization
  p = newNorm(OrgPrincipalComponents)
  p1 = newNorm(EncPrincipalComponents)
  q = newNorm(TestPrincipalComponents)
  q1 = newNorm(Enc2PrincipalComponents)

  """
  ###Mutual Info Calculation
  MIR = MICalculator(original, enc1, Labels)
  

  #print("MIR ", MIR) 
  
  ### Calculating JS between source and subspace of source, target and subspace of target#######
  js_pq = compute_js(p, p1)
  js_pq1 = compute_js(q, q1)
  
  """

  
  """###KL between two subspaces###################
  kl_loss += compute_kl(x, y)
  ###JS Between two sub spaces#################
  js_loss += compute_js(x, y)
  #print("KL Loss ", kl_loss)"""

  """
  ####MMD Loss##################################
  enc11 = np.array(enc1)
  enc21 = np.array(enc2)
  d = mmd_linear(enc11,enc21)
  """
  #####discriminator component
  real_output = model2(enc1, training=True)
  fake_output = model2(enc2, training=True)
  ts = discriminator_loss(real_output, fake_output)
  fs = generator_loss(fake_output)
  #reconstruction_error = ((sq1+sq2)/2)+tf.cast(max(0, (reg1-reg2)), tf.float32)+js_pq
  ####Loss type 7
  if lossType == 0:
    ###Mutual Info Calculation
    MIR = MICalculator(original, enc1, Labels)
    ###KL between two subspaces###################
    #kl_loss = compute_kl(x, y)
    ### Calculating JS between source and subspace of source, target and subspace of target#######
    js_pq = compute_js(p, p1)
    js_pq1 = compute_js(q, q1)
    
    
    if math.isnan(sq1):
      sq1=0
    if math.isnan(sq2):
      sq2=0
    #if math.isnan(kl_loss):
      #kl_loss=0
    if math.isnan(MIR):
      MIR=0
    if math.isnan(js_pq):
      js_pq=0
    if math.isnan(js_pq1):
      js_pq1=0
    if math.isnan(ts):
      ts=0
    if math.isnan(fs):
      fs=0
      
    
    #reconstruction_error = (0.15*sq1) + (0.3*kl_loss) + (0.15*sq2) + (0.2*MIR) + (0.1*js_pq) + (0.1*js_pq1)
    fs = tf.cast(fs, tf.float64)
    reconstruction_error = (alpha1*sq1) + (beta1*fs) + (alpha2*sq2) + (gamma1*MIR) + (delta1*js_pq) + (delta2*js_pq1)
    #reconstruction_error = sq1+ sq2 
    #print("Reconstruction error")
    #print(reconstruction_error)
  elif lossType == 1:
    ###Mutual Info Calculation
    MIR = MICalculator(original, enc1, Labels)
    ###KL between two subspaces###################
    kl_loss = compute_kl(x, y)
    ### Calculating KL between source and subspace of source, target and subspace of target#######
    kl_pq = compute_kl(p, p1)
    kl_pq1 = compute_kl(q, q1)
    if math.isnan(sq1):
      sq1=0
    if math.isnan(sq2):
      sq2=0
    if math.isnan(kl_loss):
      kl_loss=0
    if math.isnan(MIR):
      MIR=0
    if math.isnan(kl_pq):
      kl_pq=0
    if math.isnan(kl_pq1):
      kl_pq1=0
    if math.isnan(fs):
      fs=0
    fs = tf.cast(fs, tf.float64)
    reconstruction_error = sq1+ fs + sq2 +kl_pq+kl_pq1
  elif lossType == 3:
    ###Mutual Info Calculation
    MIR = MICalculator(original, enc1, Labels)
    ### Calculating KL between source and subspace of source, target and subspace of target#######
    kl_pq = compute_kl(p, p1)
    kl_pq1 = compute_kl(q, q1)
    if math.isnan(sq1):
      sq1=0
    if math.isnan(sq2):
      sq2=0
    if math.isnan(MIR):
      MIR=0
    if math.isnan(kl_pq):
      kl_pq=0
    if math.isnan(kl_pq1):
      kl_pq1=0
    if math.isnan(fs):
      fs=0
    fs = tf.cast(fs, tf.float64)
    reconstruction_error = sq1+ sq2 + MIR +kl_pq+kl_pq1+fs
  elif lossType == 4:
    ###Mutual Info Calculation
    MIR = MICalculator(original, enc1, Labels)
    ### Calculating KL between source and subspace of source, target and subspace of target#######
    kl_pq = compute_kl(p, p1)
    kl_pq1 = compute_kl(q, q1)
    if math.isnan(sq1):
      sq1=0
    if math.isnan(sq2):
      sq2=0
    if math.isnan(kl_pq):
      kl_pq=0
    if math.isnan(kl_pq1):
      kl_pq1=0
    if math.isnan(MIR):
      MIR=0
    reconstruction_error = sq1+ sq2 + kl_pq + kl_pq1 + MIR
  elif lossType == 5:
    ### Calculating KL between source and subspace of source, target and subspace of target#######
    kl_pq = compute_kl(p, p1)
    kl_pq1 = compute_kl(q, q1)
    if math.isnan(sq1):
      sq1=0
    if math.isnan(sq2):
      sq2=0
    if math.isnan(kl_pq):
      kl_pq=0
    if math.isnan(kl_pq1):
      kl_pq1=0
    reconstruction_error = sq1+ sq2 + kl_pq + kl_pq1 
  elif lossType == 6:
    ###KL between two subspaces###################
    kl_loss = compute_kl(x, y)
    ### Calculating KL between source and subspace of source, target and subspace of target#######
    kl_pq = compute_kl(p, p1)
    kl_pq1 = compute_kl(q, q1)
    if math.isnan(sq1):
      sq1=0
    if math.isnan(sq2):
      sq2=0
    reconstruction_error = sq1+ sq2 
  """
  elif lossType == 3:
    reconstruction_error = ((sq1+sq2)/2)+d
  elif lossType == 4:
    reconstruction_error = ((sq1+sq2)/2)+tf.cast((reg1-reg2), tf.float32)+ws_loss
  elif lossType == 5:
    reconstruction_error = ((sq1+sq2)/2)+kl_loss
  elif lossType == 6:
    reconstruction_error = ((sq1+sq2)/2)+js_loss
  elif lossType == 7:
    reconstruction_error = ((sq1+sq2)/2)+kl_loss2
  elif lossType == 8:
    reconstruction_error = ((sq1+sq2)/2)+js_loss2"""
  #print(reconstruction_error)
  return reconstruction_error, ts

def train( model, discModel, opt, discOpt, original, test, original_labels, lossType, alpha1, alpha2, beta1, beta2, gamma1, gamma2, delta1, delta2, loss1 = loss):
  with tf.GradientTape() as tape, tf.GradientTape() as disc_tape:
    tape.watch(model.trainable_variables)
    #disc_tape.watch(discModel.trainable_variables)
    ls, ls2 = loss1(model, discModel, original, test, original_labels, lossType, alpha1, alpha2, beta1, beta2, gamma1, gamma2, delta1, delta2)
    #print("loss is ",ls)
    if (ls != 0):
      gradients = tape.gradient(ls, model.trainable_variables)
      gradient_variables = zip(gradients, model.trainable_variables)
      opt.apply_gradients(gradient_variables)
    if (ls2 != 0):
      disc_gradients = disc_tape.gradient(ls2, discModel.trainable_variables)
      disc_gradient_variables = zip(disc_gradients, discModel.trainable_variables)
      discOpt.apply_gradients(disc_gradient_variables)
  return ls


#####################K fold Generation
def kfoldValidation(module, model, X, Y, fold):
  kfold = KFold(n_splits= fold, shuffle=False)
  mse_per_fold = []
  mae_per_fold = []
  loss_per_fold = []
  fold_no = 1
  testmodel = model
  for train, test in kfold.split(X, Y):
    rowx = tf.gather(X, train)
    rowy = tf.gather(Y, train)
    history = testmodel.fit(rowx, rowy,
                batch_size=100,
                epochs=10,
                verbose=2)
    tx = tf.gather(X, test)
    ty = tf.gather(Y, test)
    #LengthofFOlds[fold_no-1] = len(tx)
    #print(len(tx))
    #print(tx.shape)
    """ for j in range(len(tx)):
      for k in range(len(tx[j])):
        #print("j ",j, " k", k)
        FoldIndexArrayX[fold_no-1][j][k]=tx[j][k]
      FoldIndexArrayY[fold_no-1][j]=ty[j]"""
    scores = testmodel.evaluate(tx, ty, verbose=2)
    print(f'Score for fold {fold_no}: {testmodel.metrics_names[0]} of {scores[0]}; {testmodel.metrics_names[1]} of {scores[1]*100}%')
    
    loss_per_fold.append(scores[0])
    mse_per_fold.append(scores[1])
    mae_per_fold.append(scores[2])
    fold_no += 1
  return loss_per_fold, mse_per_fold, mae_per_fold

def subspace_generator(original, test, original_labels, numOfLayersI, neuronsI, activationI, alpha1, alpha2, beta1, beta2, gamma1, gamma2, delta1, delta2, epochs=1000, lr=0.01, loss_type=1000):

  intermediate_dim = 64
  if original.shape[1]<test.shape[1]:
    intermediate_dim = original.shape[1]
  else:
    intermediate_dim = test.shape[1]
  model = Autoencoder(intermediate_dim = intermediate_dim, original_dim1=original.shape[1], original_dim2=test.shape[1], numOfLayers=numOfLayersI, neurons = neuronsI, activation=activationI)
  optimizer = keras.optimizers.Adam(learning_rate=lr)
  for epoch in range(epochs):
    losses = []
    total_loss = 0
    org, tes, labels = dataGene("train", original, test, original_labels)
    org = tf.convert_to_tensor(org, dtype=tf.float64)
    tes = tf.convert_to_tensor(tes, dtype=tf.float64)
    labels = tf.convert_to_tensor(labels, dtype=tf.float64) 

    ls = train(model, optimizer, org, tes, labels, loss_type, alpha1, alpha2, beta1, beta2, gamma1, gamma2, delta1, delta2 )
    losses.append(ls)

  return model

#checkpoint_path = "training_2/cp-{epoch:04d}.ckpt"
#checkpoint_dir = path.dirname(checkpoint_path)
#"./checkpoints/train"

# Create a callback that saves the model's weights every 5 epochs


# Create a new model instance

# Save the weights using the `checkpoint_path` format
#model.save_weights(checkpoint_path.format(epoch=0))
class Objective(object):
  def __init__(self, originalP, testP, original_labelsP, loss_type, turn, chck_path):
    # Hold this implementation specific arguments as the fields of the class.
    self.org = originalP
    self.tes = testP
    self.labels = original_labelsP

    #self.org = pd.DataFrame(originalP)
    #self.tes = pd.DataFrame(testP)
    #self.labels = original_labelsP
    #self.org = tf.convert_to_tensor(self.org, dtype=tf.float32)
    #self.tes = tf.convert_to_tensor(self.tes, dtype=tf.float32)
    #self.labels = tf.convert_to_tensor(self.labels, dtype=tf.float32) 
    self.loss_type = loss_type
    self.intermediate_dim1 = 64
    self.nol = turn
    self.chck_Path = chck_path
    if self.org.shape[1]<self.tes.shape[1]:
      self.intermediate_dim1 = self.org.shape[1]
    else:
      self.intermediate_dim1 = self.tes.shape[1]
  def __call__(self, trial):
    num_layers = trial.suggest_int('num_layers', 1, 10, 2)
    neuron = trial.suggest_categorical("neuron", [10, 50, 100, 200, 300, 500, 600, 800, 900, 1000] )
    batch_size = trial.suggest_int('batch_size', low = 50, high = 300, step=50)
    lr = trial.suggest_categorical("lr", [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0])
    lr2 = trial.suggest_categorical("lr2", [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0])
    alpha1 = trial.suggest_categorical("alpha1", [.2, .30] )
    alpha2 = trial.suggest_categorical("alpha2", [.2, .30] )
    beta1 = trial.suggest_categorical("beta1", [.2, .30] )
    beta2 = 0
    gamma1 = 1- (alpha1+ alpha2+beta1)
    gamma2 = 0
    delta1 = 0
    delta2 = 0
    model = Autoencoder(intermediate_dim = self.intermediate_dim1, original_dim1=self.org.shape[1], original_dim2=self.tes.shape[1], numOfLayers = num_layers, neurons = neuron, activation="relu")
    discriminator = make_discriminator_model(intermediate_dim = self.intermediate_dim1)
    #decision = discriminator(generated_image)
    optimizer = keras.optimizers.Adam(learning_rate=lr)
    discriminator_optimizer = tf.keras.optimizers.Adam(lr2)
    cp_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath = self.savingPath+ "training_2/cp-{epoch:04d}.ckpt", 
    verbose=1, 
    save_weights_only=True,
    period=1)
    losses = []
    total_loss = 0
    ls = 0
    for epoch in range(self.nol):
      ls = train(model, discriminator, optimizer, discriminator_optimizer, self.org, self.tes, self.labels, self.loss_type, alpha1, alpha2, beta1, beta2, gamma1, gamma2, delta1, delta2 )
      trial.report(ls, epoch)
      if trial.should_prune():
          raise optuna.exceptions.TrialPruned()
    model.save_weights(self.chck_path+f"Loss Type-{self.loss_type}-Trial-{trial.number}-model")
    return ls

def finder(originalP, testP, original_labelsP, epochs, checkpoint_path, num_of_trials, loss_type, stname, storageName):
  tf.debugging.set_log_device_placement(True)	
  with tf.device('/GPU:0'):	
    #study = optuna.create_study(direction="minimize")
    study.optimize(Objective(originalP, testP, original_labelsP, loss_type, epochs, checkpoint_path), n_trials= num_of_trials, gc_after_trial=True)
    
    obj = Objective(originalP, testP, original_labelsP, test_labelsP, loss_type, epochs, checkpoint_path)
    loaded_study = optuna.load_study(study_name=stname, storage=storageName)
    #obj = Objective(targetP, target_labelsP, source_dim, epochs, checkpoint_path, reg_nuron, reg_layers, reg_rate, reg_path)
    loaded_study.optimize(obj, n_trials= num_of_trials, callbacks=[obj.callback], gc_after_trial=True)
    trial = loaded_study.best_trial
  return trial
